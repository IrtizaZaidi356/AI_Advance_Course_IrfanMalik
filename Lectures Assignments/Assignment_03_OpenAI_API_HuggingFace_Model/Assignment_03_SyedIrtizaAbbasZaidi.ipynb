{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz317N_iAEEg"
      },
      "source": [
        "# **`Assignment No # 03`**\n",
        "## **`Hope to Skills`**\n",
        "## **Free Artificial Intelligence Advance Course**\n",
        "##### **Instructor: `Sir Irfan Malik`, `Sir Dr. Sheraz`**\n",
        "\n",
        "##### ***Name: `SYED IRTIZA ABBAS ZAIDI`***\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4qMUuTJMAEEw"
      },
      "source": [
        "### **Solve the Tasks:**\n",
        "\n",
        "#### **Question 1: `Explain the followings:`**\n",
        "  - Define the context window in the context of language models.\n",
        "\n",
        "  - Explain why the model's context window is typically kept small.\n",
        "\n",
        "  - Identify and elaborate on which OpenAI language model boasts the largest context window and provide a detailed explanation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOOnPBuGItiC"
      },
      "source": [
        "##### **`Define the context window in the context of language models:`**\n",
        "\n",
        " - A `context window` is a textual range around a target token that a `large language model (LLM)` can process at the time the information is generated. Typically, the `LLM` manages the context window of a textual sequence, analyzing the passage and interdependence of its words, as well as encoding text as relevant responses.\n",
        " - As a `natural language processing` task, a context window is applicable to artificial intelligence (AI) concerns in general, along with machine learning and prompt engineering techniques, among others. For example, the words or characters in an English sentence can be segmented into multiple tokens. It's the positional encoding in generative AI that determines token placement within that textual sequence.\n",
        " - A `context window` is a critical factor in assessing the performance and determining further applications of `LLMs`. The ability to provide fast, pertinent responses based on the tokens around the target in the text history is a metric of the model's performance. A high token limit points to a higher intelligence level and larger data processing capabilities.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYTxUll-ItiE"
      },
      "source": [
        "##### **`Explain why the model's context window is typically kept small:`**\n",
        "\n",
        " - The model's context window is typically kept small in some scenarios due to computational constraints and practical considerations. Large context windows require more memory and computational resources, making training and inference computationally expensive. Here are a few reasons why a small context window is preferred in certain situations:\n",
        "\n",
        "  1) **`Computational Efficiency:`** Processing and storing information from a large context window demand more resources. A smaller window reduces the computational load, making training and inference faster and more feasible, especially in real-time applications.\n",
        "\n",
        "  2) **`Memory Constraints:`** Large language models with extensive context windows may exceed the available memory capacity, limiting the model's scalability and deployment on devices with limited resources.\n",
        "\n",
        "  3) **`Token Limitations:`** Many natural language processing tasks involve working with fixed-length sequences of tokens. A small context window helps adhere to token limitations imposed by hardware or application requirements.\n",
        "\n",
        "  4) **`Diminishing Returns:`** Beyond a certain point, increasing the context window might not significantly improve performance. There's often a balance between capturing sufficient context and avoiding unnecessary complexity.\n",
        "\n",
        "  5) **`Training Data Considerations:`** For certain tasks and datasets, a smaller context window may be sufficient to capture relevant linguistic patterns. Increasing the window size might not lead to substantial improvements and could even introduce noise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIdqR6pgItiF"
      },
      "source": [
        "##### **`Identify and elaborate on which OpenAI language model boasts the largest context window and provide a detailed explanation:`**\n",
        "\n",
        "  - **`OpenAI Language Models with Large Context Windows:`** OpenAI has developed various language models with impressive context windows, allowing them to process extensive amounts of text. Two notable models are Claude 2 by Anthropic and the OpenAI GPT-3.5 Turbo Model.\n",
        "  - **`Anthropic's Claude 2:`**\n",
        "     1) **`Context Window:`** Anthropic's chatbot, Claude 2, boasts a context window of up to 100,000 tokens, enabling it to process a vast amount of information in a single prompt.\n",
        "     2) **`Functionality:`** Users can input an entire document of approximately 75,000 words in one go when using Claude 2's API.\n",
        "     3) **`Advantages:`** The expanded context window enhances the chatbot's ability to read and analyze large volumes of text quickly and accurately, making it a powerful tool for summarization and analysis tasks.\n",
        "  \n",
        "  - **`OpenAI GPT-3.5 Turbo Model:`**\n",
        "      1) **`Context Window:`** The GPT-3.5 Turbo Model offers an impressive 16k context window, allowing it to handle substantial amounts of text for processing and summarization.\n",
        "      2) **`Usage:`** This model is specifically designed for chat interactions and is not supported by the completions endpoint, offering a unique approach to conversational AI.\n",
        "      3) **`Benefits:`** With a context window four times larger than its predecessor, this model can support around 20 pages of text in a single request, showcasing significant advancements in handling extensive textual data efficiently.\n",
        "    \n",
        "   - These advancements in context window size in language models like Claude 2 and the GPT-3.5 Turbo Model demonstrate the ongoing progress in AI technology towards more comprehensive and efficient natural language processing capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpHAWSs9AEEy"
      },
      "source": [
        "---\n",
        "\n",
        "#### **Question 2: `Please discuss your understanding on the following topics:`**\n",
        "\n",
        "  ● Main challenges and limitations associated with training and deploying and how do researchers and developers address these challenges to ensure model reliability and performance?\n",
        "  \n",
        "  ● Explain your understanding on Embeddings and Model Fine Tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_10kulTnItiS"
      },
      "source": [
        "##### **`Main challenges and limitations associated with training and deploying and how do researchers and developers address these challenges to ensure model reliability and performance?`**\n",
        "\n",
        " - **Challenges and Limitations in Training and Deploying Language Models:** Training and deploying language models present several challenges and limitations. Some of the major ones include:\n",
        "     - **`Data Collection:`** Obtaining high-quality data is crucial for training accurate models. However, collecting and labeling large volumes of data can be expensive and time-consuming.\n",
        "     - **`Computational Resources:`** Building and maintaining large language models requires considerable computational resources, including hardware and software.\n",
        "     - **`Ethical Considerations:`** There are concerns about the potential for language models to propagate biases or misinformation, necessitating careful consideration of ethics throughout the development process.\n",
        "     - **`Model Validation:`** Ensuring that models are validated against independent datasets is important to minimize the risks of overfitting and poor generalizability.\n",
        "     - **`Regulatory Frameworks:`** Developers must navigate complex legal and regulatory landscapes to ensure compliance with data protection laws and other standards.\n",
        "\n",
        " - **Addressing Challenges and Improving Reliability:** To address these challenges, researchers and developers employ strategies such as:\n",
        "     - **`Collaboration Between Domain Experts and Data Scientists:`** Close cooperation helps to identify and resolve data quality issues early in the development cycle.\n",
        "     - **`Investment in Infrastructure:`** Organizations must invest in the necessary infrastructure to support the development and deployment of large language models.\n",
        "     - **`Transparent Development Processes:`** Adopting transparent development processes can help to reduce the risk of introducing biases into models.\n",
        "     - **`External Validation:`** Regularly validating models against independent datasets can help to ensure that they remain reliable and robust.\n",
        "     - **`Compliance with Legal Standards:`** Following legal and regulatory guidelines ensures that models are compliant with data protection laws and other standards.\n",
        "\n",
        " - By implementing these strategies, developers can enhance the reliability and performance of language models, ensuring that they deliver accurate and trustworthy outputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2J0IwIfNItiT"
      },
      "source": [
        "#### **`Explain your understanding on Embeddings and Model Fine Tuning.`**\n",
        "\n",
        " - **`Embeddings:`** Embeddings represent a method of transforming text into numerical vectors, allowing language models to work with text data more effectively. They enable models to recognize patterns in text and relate words to each other, improving the overall performance of the model. To create embeddings, a model is trained on a large corpus of text, generating a dense matrix of numbers representing the meaning of every word in the vocabulary. These matrices can then be used to map new text inputs into the same vector space, facilitating comparison and pattern recognition.\n",
        "\n",
        " - **`Model Fine-Tuning:`** Fine-tuning involves adapting a pre-existing model to new data or tasks. It is a form of transfer learning, where a model is retrained on additional data to improve its performance on a specific task. Fine-tuning can involve adding new knowledge to an existing model, such as updating its weights with new information, or it can involve reconfiguring the model to work with new types of data, such as images instead of text.\n",
        " - The primary advantage of fine-tuning is that it enables models to adapt to new situations while retaining their underlying structure and learned features. This can be particularly beneficial when working with large language models, as it allows them to incorporate new information without having to relearn basic linguistic principles.\n",
        " - Overall, embeddings and fine-tuning are complementary techniques that can greatly enhance the performance of language models. Embeddings facilitate pattern recognition and comparison, while fine-tuning allows models to adapt to new situations and retain their underlying structure. Together, they enable language models to work with diverse forms of data and to continually improve their performance over time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdJlBZGSAEEz"
      },
      "source": [
        "---\n",
        "\n",
        "#### **Question 3: `Design a sentiment analysis application using OpenAI's models to determine the sentiment of a given text (positive, negative, or neutral)?`**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJnKOiTDL5P2",
        "outputId": "03ba4f4a-d8dc-4595-e930-a58de5724fca"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Installing collected packages: h11, httpcore, httpx, openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 openai-1.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "api_key = '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'\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    openai.api_key = api_key\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"\"\"You are trained to analyze and detect the sentiment of given text.\n",
        "                                        If you're unsure of an answer, you can say \"not sure\" and recommend users to review manually.\"\"\"},\n",
        "        {\"role\": \"user\", \"content\": f\"\"\"Analyze the following text and determine if the sentiment is: positive or negative.\n",
        "                                        Return answer in single word as either positive or negative: {text}\"\"\"}\n",
        "        ]\n",
        "\n",
        "    # Call the OpenAI sentiment analysis model\n",
        "    response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n",
        "                                              messages=messages,\n",
        "                                              max_tokens=1,\n",
        "                                              n=1,\n",
        "                                              temperature=0)\n",
        "\n",
        "    # Determine the sentiment based on the model's response\n",
        "    if 'Positive' in response.choices[0].text:\n",
        "        return 'Positive'\n",
        "    elif 'Negative' in response.choices[0].text:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n"
      ],
      "metadata": {
        "id": "B5qNnSCNJG6M"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the function with a sample text\n",
        "sample_text = \"I love this product, it works great!\"\n",
        "response = analyze_sentiment(sample_text)\n",
        "print(sample_text, \": The Sentiment is \", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "4HYVR8nzTlo0",
        "outputId": "830830d0-35c3-4d4e-e224-892c06d2afb7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-a1f689d32089>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Test the function with a sample text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0msample_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"I love this product, it works great!\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\": The Sentiment is \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-6395324d626e>\u001b[0m in \u001b[0;36manalyze_sentiment\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Call the OpenAI sentiment analysis model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     response = openai.ChatCompletion.create(model=\"gpt-3.5-turbo\",\n\u001b[0m\u001b[1;32m     16\u001b[0m                                               \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                                               \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_resources/abstract/engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[0;34m(self, result, stream)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[0;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jTkhYqADItiT"
      },
      "source": [
        "---\n",
        "\n",
        "#### **Question 4: `Design a Python program that compares two text inputs and determines their similarity using the OpenAI API.`**\n",
        "\n",
        "**Instructions:**\n",
        "  - Prompt the user to input two texts for comparison.\n",
        "  - Utilize the OpenAI GPT-3.5-turbo language model to analyze the similarity between the provided texts.\n",
        "  - Display the similarity score to the user, indicating how similar the two texts are.\n",
        "  - Handle errors gracefully, such as empty inputs or API request failures.\n",
        "  - Ensure the program is properly documented and easy to understand.\n",
        "  - Optionally, provide additional information to the user about the comparison process or the meaning of the similarity score.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install matplotlib Pillow openai==0.28\n",
        "#!pip install openai --upgrade\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNPI4DuAorWa",
        "outputId": "a5795674-befd-4550-86f3-8bd3a034e2aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Collecting openai\n",
            "  Using cached openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai) (0.27.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (2.6.1)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai) (1.3.0)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->openai) (2.16.2)\n",
            "Installing collected packages: openai\n",
            "  Attempting uninstall: openai\n",
            "    Found existing installation: openai 0.28.0\n",
            "    Uninstalling openai-0.28.0:\n",
            "      Successfully uninstalled openai-0.28.0\n",
            "Successfully installed openai-1.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Set your OpenAI API key here\n",
        "openai.api_key = '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'\n",
        "\n",
        "def compare_texts(text1, text2):\n",
        "    try:\n",
        "        # Use GPT-3.5-turbo to analyze the similarity between the provided texts\n",
        "        response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-16k\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": text1},\n",
        "                {\"role\": \"assistant\", \"content\": text2},\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        similarity_score = response['choices'][0]['message']['content']\n",
        "        print(f\"\\nSimilarity Score: {similarity_score}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        # Handle API request failures\n",
        "        print(f\"Error during API request: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Prompt the user to input two texts for comparison\n",
        "    text1 = input(\"Enter the first text: \")\n",
        "    text2 = input(\"Enter the second text: \")\n",
        "\n",
        "    # Check for empty inputs\n",
        "    if not text1 or not text2:\n",
        "        print(\"Error: Please provide non-empty texts for comparison.\")\n",
        "    else:\n",
        "        # Compare the provided texts\n",
        "        compare_texts(text1, text2)\n"
      ],
      "metadata": {
        "id": "a1l_hTOkUdDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00d9420f-787f-40b0-b2a2-ff18812f946c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the first text: what is the real estate\n",
            "Enter the second text: how to implementation of ai in real estate\n",
            "Error during API request: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DvCcvdpxItiU"
      },
      "source": [
        "---\n",
        "\n",
        "#### **Question 5: `Develop a Python program that generates a blog post based on a user-provided topic. The program should use OpenAI's API to create a complete blog post including a title, a description (approximately 300 words), related keywords, SEO meta title, SEO meta description, and a corresponding image. The program should interact with the user through the input() function for topic input and display the generated content within the Colab notebook.`**\n",
        "\n",
        "**Hints:**\n",
        " - Review OpenAI's API documentation, and use the appropriate models for the task.\n",
        " - Practice prompt engineering to guide the AI for each specific content piece (title, description, keywords, meta title, and description).\n",
        " - For image generation, describe the kind of image that would be suitable for the blog post and visualize it accordingly.\n",
        " - Implement robust error handling for API interactions.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# Set your OpenAI API key here\n",
        "openai.api_key = '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'\n",
        "\n",
        "def generate_blog_post(topic):\n",
        "    try:\n",
        "        # Generate blog post title\n",
        "        title_prompt = f\"Create a catchy title for a blog post about {topic}:\"\n",
        "        title_response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-16k\",\n",
        "            prompt=title_prompt,\n",
        "            max_tokens=50\n",
        "        )\n",
        "        title = title_response['choices'][0]['text'].strip()\n",
        "\n",
        "        # Generate blog post description\n",
        "        description_prompt = f\"Write a blog post about {topic}. Include an engaging description (approximately 300 words):\"\n",
        "        description_response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            prompt=description_prompt,\n",
        "            max_tokens=400\n",
        "        )\n",
        "        description = description_response['choices'][0]['text'].strip()\n",
        "\n",
        "        # Generate related keywords\n",
        "        keywords_prompt = f\"List 5 keywords related to {topic}:\"\n",
        "        keywords_response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-16k\",\n",
        "            prompt=keywords_prompt,\n",
        "            max_tokens=100\n",
        "        )\n",
        "        keywords = keywords_response['choices'][0]['text'].strip()\n",
        "\n",
        "        # Generate SEO meta title\n",
        "        meta_title_prompt = f\"Create an SEO-friendly title for a blog post about {topic}:\"\n",
        "        meta_title_response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            prompt=meta_title_prompt,\n",
        "            max_tokens=50\n",
        "        )\n",
        "        meta_title = meta_title_response['choices'][0]['text'].strip()\n",
        "\n",
        "        # Generate SEO meta description\n",
        "        meta_description_prompt = f\"Write an SEO-friendly meta description for a blog post about {topic}:\"\n",
        "        meta_description_response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            prompt=meta_description_prompt,\n",
        "            max_tokens=150\n",
        "        )\n",
        "        meta_description = meta_description_response['choices'][0]['text'].strip()\n",
        "\n",
        "        # Display generated content\n",
        "        print(f\"\\nTitle: {title}\\n\")\n",
        "        print(f\"Description: {description}\\n\")\n",
        "        print(f\"Keywords: {keywords}\\n\")\n",
        "        print(f\"SEO Meta Title: {meta_title}\\n\")\n",
        "        print(f\"SEO Meta Description: {meta_description}\\n\")\n",
        "\n",
        "        # Generate and display a placeholder image\n",
        "        image_description = f\"An image related to {topic}\"\n",
        "        generate_image(image_description)\n",
        "\n",
        "    except openai.error.OpenAIError as e:\n",
        "        # Handle API request failures\n",
        "        print(f\"Error during API request: {e}\")\n",
        "\n",
        "def generate_image(description):\n",
        "    # Placeholder image generation (replace with actual image generation logic)\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.text(0.5, 0.5, description, fontsize=12, ha='center', va='center')\n",
        "    plt.axis('off')\n",
        "    img_data = io.BytesIO()\n",
        "    plt.savefig(img_data, format='png')\n",
        "    img_data.seek(0)\n",
        "    img = Image.open(img_data)\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Prompt the user to input a topic for the blog post\n",
        "    user_topic = input(\"Enter the topic for the blog post: \")\n",
        "\n",
        "    # Check for empty input\n",
        "    if not user_topic:\n",
        "        print(\"Error: Please provide a topic for the blog post.\")\n",
        "    else:\n",
        "        # Generate the blog post content\n",
        "        generate_blog_post(user_topic)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kin8cd1jppyE",
        "outputId": "2c807219-d0c6-49c2-f7c9-30f3741e7c7a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the topic for the blog post: deep learning\n",
            "Error during API request: You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unmLaCHYItiU"
      },
      "source": [
        "---\n",
        "\n",
        "#### **Question 6: `What is Hugging Face and why do we briefly explore it?`**\n",
        "  - Explore Transformers\n",
        "  - Explore Pipeline\n",
        "  - Explore Hugging Face API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9VYfMAiyItiU"
      },
      "source": [
        "**`Understanding Hugging Face, Transformers, Pipeline, and Hugging Face API:`** Hugging Face is a prominent machine learning and data science platform that serves as a community hub for building, deploying, and training machine learning models. Here is an exploration of key components associated with Hugging Face:\n",
        "\n",
        "**`Hugging Face Overview:`**\n",
        "    \n",
        "  - **`Platform Features:`** Hugging Face offers a range of functionalities such as creating organizations or private repositories, initiating models and datasets, accessing ML resources, and exploring the latest trends within the community.\n",
        "    \n",
        "  - **`Model Sharing:`** Users can upload, share, and discover machine learning models through Spaces and the Hugging Face Transformers library, enabling collaborative model development and deployment.\n",
        "    \n",
        "  - **`Data Sets:`** The platform hosts over 30,000 datasets that can be used for training ML models across various tasks in natural language processing (NLP), computer vision, audio processing, and more.\n",
        "    \n",
        "  - **`Community Collaboration:`** Hugging Face fosters an open-source community approach where data scientists, researchers, and ML engineers can exchange ideas, collaborate on projects, and contribute to open-source initiatives.\n",
        "\n",
        "**`Transformers Library:`**\n",
        "    \n",
        "  - **`Pre-Trained Models:`** The Transformers library by Hugging Face provides immediate access to over 20,000 pre-trained models based on transformer architecture for tasks like text classification, information extraction, question answering, translation, speech recognition, image classification, and more.\n",
        "    \n",
        "  - **`Tokenization:`** Tokenizers in the Transformers library break down text into tokens for model input preparation and facilitate text preprocessing tasks like cleaning and embedding.\n",
        "    \n",
        "  - **`Pipelines:`** Transformers offer an easy-to-use API through the pipeline() method for performing inference across various NLP tasks without the need for extensive coding.\n",
        "\n",
        "**`Hugging Face API:`**\n",
        "    \n",
        "  - **`Model Deployment:`** Users can fine-tune and train deep learning models using Hugging Face's API tools for hosting demos, conducting research projects, developing business applications, evaluating ML models, and more.\n",
        "    \n",
        "  - **`Enterprise Solutions:`** Hugging Face provides enterprise-grade solutions with enhanced security features and dedicated customer support for organizations requiring advanced AI capabilities.\n",
        "    \n",
        "  - **`Model Accessibility:`** The API allows users to create interactive demos of machine learning models in-browser, facilitating model showcasing and testing processes.\n",
        "\n",
        "By exploring Hugging Face's ecosystem encompassing the platform features, Transformers library functionalities, and API capabilities, users can leverage a comprehensive set of tools to streamline model development, deployment, collaboration within the AI community."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYJ6DAb6ItiV"
      },
      "source": [
        "---\n",
        "\n",
        "#### **Question 7: `Build a simple text sentiment analyzer that categorizes sentences as positive,negative, or neutral.`**\n",
        "\n",
        "**Hint:**\n",
        " - Pick any pre-trained HuggingFace sentiment analysis model from the Hugging Face."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install transformers"
      ],
      "metadata": {
        "id": "gUNH-MkiusYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def analyze_sentiment(sentence):\n",
        "    # Load pre-trained sentiment analysis model\n",
        "    sentiment_analyzer = pipeline('sentiment-analysis')\n",
        "\n",
        "    # Analyze the sentiment of the given sentence\n",
        "    result = sentiment_analyzer(sentence)\n",
        "\n",
        "    # Extract the label and score\n",
        "    label = result[0]['label']\n",
        "    score = result[0]['score']\n",
        "\n",
        "    return label, score\n",
        "\n",
        "def main():\n",
        "    # Example sentences for sentiment analysis\n",
        "    sentences = [\n",
        "        \"I love this product! It's amazing.\",\n",
        "        \"The weather is terrible today.\",\n",
        "        \"we chose a neutral colour scheme\",\n",
        "    ]\n",
        "\n",
        "    # Analyze sentiment for each sentence\n",
        "    for sentence in sentences:\n",
        "        label, score = analyze_sentiment(sentence)\n",
        "        print(f\"Sentence: '{sentence}'\")\n",
        "        print(f\"Sentiment: {label} with confidence score: {score:.4f}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBSx19Ioulzu",
        "outputId": "f8a77d14-758b-477f-fb98-c1608f503020"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: 'I love this product! It's amazing.'\n",
            "Sentiment: POSITIVE with confidence score: 0.9999\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: 'The weather is terrible today.'\n",
            "Sentiment: NEGATIVE with confidence score: 0.9987\n",
            "\n",
            "Sentence: 'we chose a neutral colour scheme'\n",
            "Sentiment: NEGATIVE with confidence score: 0.9985\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-KgyB-aItiV"
      },
      "source": [
        "---\n",
        "\n",
        "#### **Question 8: `You have a corpus of news articles containing information about different companies and their locations. Use HuggingFace model, develop a pipeline that extracts the names and locations of these companies from the text.`**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def extract_entities(text):\n",
        "    # Load the pre-trained NER model\n",
        "    ner_pipeline = pipeline(\n",
        "        \"question-answering\",\n",
        "        model=\"chriskim2273/IOTNation_CompanyName_AND_Location_AND_Series_Extraction_QA_Model_1.9_DistilBert_UNK_DATASET\",\n",
        "        tokenizer=\"chriskim2273/IOTNation_CompanyName_AND_Location_AND_Series_Extraction_QA_Model_1.9_DistilBert_UNK_DATASET\"\n",
        "    )\n",
        "\n",
        "    # Extract entities from the text\n",
        "    result = ner_pipeline({\n",
        "        'question': 'What are the names and locations of the companies?',\n",
        "        'context': text\n",
        "    })\n",
        "\n",
        "    # Extract relevant information from the result\n",
        "    companies = result['answer']\n",
        "\n",
        "    return companies\n",
        "\n",
        "def main():\n",
        "    # Example news article containing information about companies and locations\n",
        "    news_article = \"\"\"\n",
        "    Apple Inc. is planning to open a new office in San Francisco. Microsoft Corporation is expanding its operations in Seattle.\n",
        "    Alphabet Inc., the parent company of Google, is headquartered in Mountain View, California.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract entities from the news article\n",
        "    extracted_entities = extract_entities(news_article)\n",
        "\n",
        "    # Print the extracted companies and locations\n",
        "    print(\"Extracted Companies and Locations:\")\n",
        "    print(extracted_entities)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSfeD9j5xCzP",
        "outputId": "2328ffe9-5a8c-4836-8cf6-aadb627e7b4b"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted Companies and Locations:\n",
            "Mountain View\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXe2aNipItiV"
      },
      "source": [
        "---\n",
        "\n",
        "#### **Question 9: `Create a text generator that produces creative text continuations based on your input prompts.`**\n",
        "\n",
        "**Hint:**\n",
        " - Select a pre-trained model suited for text generation from the Hugging Face"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "from transformers import pipeline\n",
        "\n",
        "# Load the pre-trained model for text generation\n",
        "generator = pipeline('text-generation', model='Moxis/Harry_Potter_text_generation')\n",
        "\n",
        "# Input prompt for the text generation\n",
        "input_prompt = \"Harry Potter and Hermione Granger walked into the Forbidden Forest\"\n",
        "\n",
        "# Generate creative text continuation based on the input prompt\n",
        "generated_text = generator(input_prompt, max_length=100, num_return_sequences=1)\n",
        "\n",
        "# Print the generated text\n",
        "print(generated_text[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3w5-EVe1gGO",
        "outputId": "cda80b8a-8e91-4708-cd9f-bea5a6de8e9e"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Harry Potter and Hermione Granger walked into the Forbidden Forest together, as they had planned, looking for the Elder Wand.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dpy6XsLyItiW"
      },
      "source": [
        "---\n",
        "\n",
        "#### **Question 10: `Use any one hugging face model in your project. - Write a python code that uses HuggingFace tokenizers library to tokenize a given sentence using the BERT tokenizers.`**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Input sentence to tokenize\n",
        "input_sentence = \"Tokenize this sentence using BERT tokenizer.\"\n",
        "\n",
        "# Tokenize the input sentence using the BERT tokenizer\n",
        "tokenized_input = tokenizer(input_sentence, return_tensors='pt')\n",
        "\n",
        "# Print the tokenized input\n",
        "print(tokenized_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijo_WOGT3_ZG",
        "outputId": "b86142c2-7d18-459d-ccf8-a27a39d6ba55"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[  101, 19204,  4697,  2023,  6251,  2478, 14324, 19204, 17629,  1012,\n",
            "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}